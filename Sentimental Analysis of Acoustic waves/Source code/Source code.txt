<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>Minor Project </title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" crossorigin="anonymous">
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz" crossorigin="anonymous"></script>
        <style>
            #audioFile{
                width: 200px;
                padding: 10px;
                border: 1px solid #ccc;
                background-color: #f1f1f1;
                color: #222;
            }
            .mla{
                margin-left: 50px!important;
                padding-top: 5px!important;
            }
            .hidden{
                display: none;
            }
            h4, h5{
                display: inline;
            }
        </style>
    </head>
    <body>
        <div class='container mt-5'>
            <h1>Sentimental Analysis of Acoustic Waves </h1> 
                <form method="post" enctype="multipart/form-data" class='mt-3 pt-2'>
                    {% csrf_token %}
                    <input type="file" id="audioFile" class='mlb' name="mysoundfile">
                    <audio controls id="audioPlayer" class='mla'>
                        <source src={{sound}} type="audio/wav">
                      </audio>
                    <br> <br>
                    <button class='btn btn-success' type="submit">Predict</button> 
                </form>  <br>
                <h4> Predicted Emotion: </h4> <h5> &nbsp; {{emotion}}</h5> 
        </div> 
                <script>
                    const audioFileInput = document.getElementById('audioFile');
            const audioPlayer = document.getElementById('audioPlayer');

            audioFileInput.addEventListener('change', function() {
                const file = audioFileInput.files[0];
                const fileURL = URL.createObjectURL(file);
                audioPlayer.src = fileURL;
            });
        </script>
    </body>
</html>









from django.shortcuts import render
from django.http import HttpResponse, HttpResponseRedirect
from django.urls import reverse
import pickle
import os
import sklearn
import soundfile
import librosa
import numpy as np
import matplotlib.pyplot as plt
import io
import base64


# Create your views here.

def extract_features(filename, mfcc, chroma, mel):
    with soundfile.SoundFile(filename) as sound_file:
        X = sound_file.read(dtype='float32')
        sample_rate = sound_file.samplerate
        if chroma:
            stft=np.abs(librosa.stft(X))
            result=np.array([])
        if mfcc:
            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)
            result=np.hstack((result, mfccs))
        if chroma:
            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)
            result=np.hstack((result, chroma))
        if mel:
            mel=np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)
            result=np.hstack((result, mel))
    return result

def loadModel():
    modulePath = os.path.dirname(_file_)
    filepath = os.path.join(modulePath, 'gbc_model.pkl')
    with open(filepath, 'rb') as f:
        model=pickle.load(f)
    return model

def predict(data):
    model = loadModel()
    emotion = model.predict(extract_features(data, mfcc=True, chroma=True, mel=True).reshape(1,-1))
    return emotion


def take_input(request):
    context = {}
    if request.method == 'POST':
        inputsound = request.FILES['mysoundfile']
        emotion = predict(inputsound)
        context = {
            'emotion': emotion,
            }
    return render(request, 'upload_form.html', context)





!pip install soundfile
!pip install librosa
import librosa
import soundfile
import os, glob, pickle
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
def extract_feature(file_name, mfcc, chroma, mel):
        with soundfile.SoundFile(file_name) as sound_file:
            X = sound_file.read(dtype="float32")
            sample_rate=sound_file.samplerate
            if chroma:
                stft=np.abs(librosa.stft(X))
                result=np.array([])
            if mfcc:
                mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)
                result=np.hstack((result, mfccs))
            if chroma:
                chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)
                result=np.hstack((result, chroma))
            if mel:
                mel=np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)
                result=np.hstack((result, mel))
        return result
emotions={
  '01':'neutral',
  '02':'calm',
  '03':'happy',
  '04':'sad',
  '05':'angry',
  '06':'fearful',
  '07':'disgust',
  '08':'surprised'
}
observed_emotions=['happy', 'fearful', 'surprised','sad','angry']

def load_data(test_size=0.2):
    x,y=[],[]
    for file in glob.glob(os.path.join(os.getcwd(),"speech-emotion-recognition-ravdess-data\\Actor_*\\*.wav")):
        file_name=os.path.basename(file)
        emotion=emotions[file_name.split("-")[2]]
        if emotion not in observed_emotions:
            continue
        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)
        x.append(feature)
        y.append(emotion)
    return train_test_split(np.array(x), y, test_size=test_size)

x_train,x_test,y_train,y_test=load_data(test_size=0.1)
print((x_train.shape[0], x_test.shape[0]))
print(f'Features extracted: {x_train.shape[1]}')

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

gbc = GradientBoostingClassifier()

gbc.fit(x_train, y_train)

y_pred = gbc.predict(x_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

pickle.dump(gbc, open('gbc_model.pkl', 'wb'))


import pickle
loaded_model = pickle.load(open('gbc_model.pkl', 'rb'))
print(loaded_model.predict(extract_feature('one.wav', mfcc=True, chroma=True, mel=True).reshape(1, -1)))






